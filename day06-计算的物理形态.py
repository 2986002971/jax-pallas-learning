# %% [markdown]
# # Day 6: 硅基异构与计算的物理形态 (Hardware Execution Models)
#
# **核心叙事**：
# 编译器（XLA）不仅是数学公式的翻译官，更是底层硬件的调度员。之前我们讨论的 Vmap 和 Sharding 属于“逻辑视图”，而本章我们将切换到“物理视图”。
# 我们将深入对比 GPU 与 TPU/NPU 的微观架构，理解为什么同样的 JAX 代码，在不同硬件上会被编译成截然不同的指令流。
#
# ---
#
# ### Part 1: 两种计算范式的演进
#
# 在高性能计算领域，存在两种解决核心矛盾（计算 vs 访存）的哲学：
#
# 1.  **GPU (通用并行架构)**：
#     *   **核心特征**：SIMT (Single Instruction Multiple Thread)。
#     *   **设计哲学**：**以大量线程并发来掩盖访存延迟 (Latency Hiding)**。它假设控制流是复杂的，内存访问是随机的，因此依靠硬件调度器在运行时动态分配资源。
#     *   **比喻**：一个高度自动化的**大型调度中心**。
#
# 2.  **TPU/NPU (领域专用架构 - DSA)**：
#     *   **核心特征**：Systolic Array (脉动阵列) + Explicit Memory (显式内存)。
#     *   **设计哲学**：**以确定的数据流来实现极致吞吐 (Throughput Oriented)**。它假设计算是规整的（矩阵乘法），数据流向是可预知的，因此将调度压力转移给编译器，硬件专注于计算。
#     *   **比喻**：一条精密咬合的**全自动流水线**。
#
# ---
#
# 好的，这是完善后的 **Day 4.2** 章节。我们将像剥洋葱一样，由外向内解构 GPU 的存储层级，并阐述硬件如何利用大规模线程并发来掩盖这种层级带来的延迟。
#
# ---
#
# ### Part 2: GPU —— 掩盖延迟的艺术 (The Art of Latency Hiding)
#
# **核心矛盾**：
# 在现代 GPU 上，计算力（FLOPS）是廉价的，而数据搬运（Bandwidth & Latency）是昂贵的。
# A100 的 FP16 算力高达 312 TFLOPS，但显存带宽只有 1.6 TB/s。更糟糕的是，从显存读取数据的延迟通常高达 **400-800 个时钟周期**。
# 如果 GPU 像 CPU 一样顺序执行指令，它 99% 的时间都在等待数据。GPU 的解决之道是：**极其深度的存储层级** 加上 **极其暴力的多线程切换**。
#
# #### **1. 存储金字塔：从 HBM 到 寄存器**
#
# 让我们追踪一个数据包从显存流向计算单元的旅程。这是理解性能瓶颈（如 Bandwidth Bound vs Compute Bound）的基础。
#
# *   **最外层: Global Memory (HBM - 高带宽显存)**
#     *   **角色**：巨大的离片（Off-chip）仓库。
#     *   **特征**：容量大（40GB/80GB），但距离核心最远，延迟最高。
#     *   **关键约束 - 合并访存 (Coalesced Access)**：
#         显存控制器不是按字节读取的，而是按“事务（Transaction）”读取（通常是 32 或 128 字节）。
#         *   *Good Case*：如果一个 Warp 里的 32 个线程正好连续读取 `addr` 到 `addr+31` 的数据，硬件只需发射 **1 次** 显存事务。
#         *   *Bad Case*：如果 32 个线程乱序读取（例如跳跃访问），硬件可能要发射 **32 次** 显存事务。有效带宽瞬间跌至 1/32。
#         *   *JAX/XLA 的工作*：自动向量化（Vectorization）的一个核心目标，就是保证生成的内存读取指令是连续对齐的。
#
# *   **中间层: L2 Cache (二级缓存)**
#     *   **角色**：所有 SM（流多处理器）共享的中转站，连接 HBM 和 SM 的枢纽。
#     *   **特征**：片上（On-chip），速度比 HBM 快得多。
#     *   **作用**：
#         1.  **减少 HBM 压力**：当多个 SM 读取相同的权重（Weights）时，L2 只需从 HBM 读一次，然后分发给各家。
#         2.  **原子操作中心**：由于它是全局可见的，跨 Block 的原子操作（如某些 Global Reduce）通常在这里协调。
#
# *   **近核层: L1 Cache vs. Shared Memory (双重人格的 SRAM)**
#     *   **位置**：每个 SM 内部的高速存储块（SRAM）。这是离计算最近的“公共区域”。
#     *   **双重人格**：物理上它们通常是同一块 SRAM，但逻辑上被切分为两部分（可配置）：
#         1.  **L1 Cache (隐式/自动)**：硬件自动管理。用来缓存从 L2 读来的数据，以此处理那些不规则的、难以预测的内存访问。它是“保底机制”。
#         2.  **Shared Memory (显式/手动)**：由程序员（或 XLA 编译器）显式控制。
#             *   在矩阵乘法中，编译器会将数据块（Tile）显式加载到这里。它像是一个**可编程的 L1 Cache**，保证了数据一旦进来，就不会被随机的 Cache 策略踢出去。
#     *   **Bank Conflict**：Shared Memory 被划分为 32 个 Bank。如果多个线程同时访问同一个 Bank 的不同地址，访问会变成串行，导致流水线停顿。
#
# *   **核心层: Registers (寄存器)**
#     *   **角色**：计算的“工作台”。
#     *   **特征**：**唯一**能进行数学运算的地方。数据必须先从 RAM/Cache 加载到寄存器，才能被 ALU 或 Tensor Core 处理。
#     *   **代价**：它是线程私有的。
#
# ---
#
# #### **2. 执行模型：SIMT 与 快速上下文切换**
#
# 有了存储层级，GPU 依然面临一个问题：即使是从 L2 读数据，也需要几十个周期。怎么填补这段空白？
#
# 答案是 **SIMT (Single Instruction Multiple Thread)**。
#
# *   **Warp (线程束)**：
#     GPU 并不调度单个线程，而是将 **32 个线程** 捆绑成一个 Warp。它们共用同一个程序计数器（PC），锁步执行。
#
# *   **延迟掩盖 (Latency Hiding)**：
#     这是 GPU 架构的灵魂。
#     *   想象一个 SM 上驻留了 16 个 Warp。
#     *   **Warp 0** 执行到了 `LOAD R1, [Addr]`。这需要等 500 个周期。
#     *   Warp Scheduler **立刻**（0 周期开销）把 Warp 0 挂起，切换到 **Warp 1**。
#     *   如果 Warp 1 也要等内存，就切到 Warp 2...
#     *   只要驻留的 Warp 足够多，总有一个是“这就绪”的，ALU 就永远不会空转。
#
#     > **公式**：为了掩盖 $L$ 周期的延迟，如果每条指令吞吐是 $T$，你需要 $L \times T$ 的并发指令在飞行中（Little's Law）。
#
# #### **3. 编译器的博弈：寄存器压力 vs. Occupancy**
#
# 这里引出了 XLA 编译器最纠结的权衡：
#
# *   为了让单个线程跑得快（做更复杂的融合、存更多的中间结果），编译器倾向于给每个线程分配**更多寄存器**。
# *   **但是**，SM 的寄存器堆总大小是固定的（例如 64KB）。
# *   **后果**：
#     *   如果每个线程用的寄存器多了，SM 上能容纳的 Warp 总数（**Occupancy**）就少了。
#     *   Warp 少了，由于“延迟掩盖”机制，当所有 Warp 都在等内存时，GPU 就会彻底停摆（Stall）。
#
# **总结**：
# GPU 的高性能不仅仅源于堆砌核心，更源于它在**访存延迟**和**大规模并发**之间维持的一种微妙平衡。
# 写出好代码（或生成好代码）的关键，在于**善用 Shared Memory 减少 HBM 访问**，同时**控制寄存器用量以维持足够的并发度**。
#
# ---
#
# ### Part 3: TPU/NPU —— 静态流水线的极致
#
# 与 GPU 的动态调度不同，TPU 和 Ascend 等 NPU 采用的是**静态规划**的路线。硬件剔除了复杂的缓存控制和分支预测逻辑，将晶体管全部用于计算。
#
# #### **1. 核心计算：脉动阵列 (Systolic Array)**
#
# 这是 TPU 区别于 CPU/GPU 最本质的结构。
#
# *   **机制**：由成百上千个乘加单元（MACs）组成的二维网格（如 TPU v4 是 $128 \times 128$）。
# *   **Weight Stationary (权重驻留)**：
#     在矩阵乘法 $C = A \times B$ 中，权重 $B$ 被预先加载并“固定”在阵列的每个节点上。输入数据 $A$ 从左侧流入，部分和（Partial Sums）在节点间传递。
# *   **优势**：数据一旦从片上内存读出，会在阵列内部复用成百上千次，极大降低了对内存带宽的压力。
#
# #### **2. 显式内存管理 (Explicit Memory Management)**
#
# 这是编译器最头疼，但也是性能收益最大的地方。
#
# *   **Scratchpad Memory (VMem / UB)**：
#     NPU 没有硬件管理的 Cache（或者 Cache 不作为主要数据通路）。它拥有巨大的片上高速暂存器（TPU 的 VMem，Ascend 的 Unified Buffer）。
#     *   **显式搬运**：数据不会自动出现在 VMem 里。编译器必须生成显式的 DMA 指令，将数据从 HBM 搬运到 VMem。
#     *   **确定性**：因为没有 Cache Miss 带来的抖动，程序的执行时间是严格可预测的。
#
# #### **3. 指令流：VLIW 与软件流水 (Software Pipelining)**
#
# TPU 的指令集通常具有 **VLIW (超长指令字)** 的特征。一条指令包可能同时包含：
# 1.  **MXU 指令**：启动脉动阵列计算。
# 2.  **VPU 指令**：向量单元进行 Activation/Ewise 操作。
# 3.  **DMA 指令**：搬运下一块数据。
#
# **双缓冲 (Double Buffering) 策略**：
# XLA 编译器必须在编译期计算出完美的流水线：
# *   在 $T$ 时刻，MXU 计算 Buffer A 的数据。
# *   同时在 $T$ 时刻，DMA 引擎将下一批数据预取到 Buffer B。
# *   如果编译器计算失误（例如 DMA 搬运慢了），MXU 就会发生 **Stall (停滞)**，硬件利用率瞬间归零。
#
# ---
#
# ### Part 4: 编译器的最终裁决 (Why XLA?)
#
# 理解了上述硬件差异，就能明白 XLA (Accelerated Linear Algebra) 在 JAX 生态中的核心地位。
#
# #### **1. 为什么动态 Shape 是 TPU 的天敌？**
# *   **GPU**：如果 Batch Size 从 32 变到 31，GPU 只是少跑一个线程，影响甚微。
# *   **TPU**：脉动阵列是物理固定的（例如 128x128）。如果数据维度不匹配，必须进行 **Padding (填充)**。
#     *   如果 Shape 频繁变化，编译器需要不断重新规划内存布局和 Padding 策略，导致 Re-compilation，且执行效率低下（大量算力浪费在算 0 上）。
#
# #### **2. 算子融合 (Fusion) 的不同侧重**
# *   **GPU Fusion**：主要是为了减少 Global Memory 读写，把中间结果这就留在寄存器或 Shared Memory 中。
# *   **TPU Fusion**：除了减少 HBM 访问，更重要的是**匹配流水线节拍**。将 Element-wise 操作（如 ReLU, Add）融合到矩阵乘法后面，利用向量单元（Vector Unit）在数据流出脉动阵列的瞬间顺手处理掉，实现“零开销”。
#
# ---
#
# ### 总结
#
# *   **GPU 执行模型**：基于 **SIMT**。依靠 **Occupancy** 和 **Warp Switching** 来动态对抗内存延迟。代码稍乱一点，硬件也能“尽力而为”。
# *   **TPU 执行模型**：基于 **Dataflow**。依靠 **DMA 流水线** 和 **脉动阵列** 来实现确定性的吞吐。代码必须严格静态，编译器拥有绝对的控制权。
